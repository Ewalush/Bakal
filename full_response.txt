doc_bot_py/main2.py



150364



DeepSeek-V3 Technical Report DeepSeek-AI research@deepseek.com Abstract We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec- tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. MMLU-Pro (EM) GPQA-Diamond (Pass@1) MATH 500 (EM) AIME 2024 (Pass@1) Codeforces (Percentile) SWE-bench Verified (Resolved) 0 20 40 60 80 100 Accuracy / Percentile (%) 75.9 59.1 90.2 39.2 51.6 42.0 66.2 41.3 74.7 16.7 35.6 22.6 71.6 49.0 80.0 23.3 24.8 23.8 73.3 51.1 73.8 23.3 25.3 24.5 72.6 49.9 74.6 9.3 23.6 38.8 78.0 65.0 78.3 16.0 20.3 50.8 DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022 Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts. arXiv:2412.19437v2 [cs.CL] 18 Feb 2025 Contents 1 Introduction 4 2 Architecture 6 2.1 Basic Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.1 Multi-Head Latent Attention . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing . . . . . . . . . . 8 2.2 Multi-Token Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 Infrastructures 11 3.1 Compute Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Training Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2.1 DualPipe and Computation-Communication Overlap . . . . . . . . . . . . 12 3.2.2 Efficient Implementation of Cross-Node All-to-All Communication . . . . 13 3.2.3 Extremely Memory Saving with Minimal Overhead . . . . . . . . . . . . . 14 3.3 FP8 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.3.1 Mixed Precision Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.3.2 Improved Precision from Quantization and Multiplication . . . . . . . . . 16 3.3.3 Low-Precision Storage and Communication . . . . . . . . . . . . . . . . . 18 3.4 Inference and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.4.1 Prefilling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.4.2 Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.5 Suggestions on Hardware Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.5.1 Communication Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.5.2 Compute Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4 Pre-Training 21 4.1 Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.3 Long Context Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.4 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .



Embeddings 
 [-7.36544877e-02 -5.92405200e-02  5.75506799e-02  7.15857968e-02
  1.54401381e-02 -1.63372029e-02 -7.57386759e-02 -7.95339933e-04
 -2.40842719e-02 -6.18494041e-02 -1.24292076e-01 -1.00587510e-01
 -2.54351925e-02 -4.82991971e-02  1.18151708e-02  2.85590813e-02
  5.89606278e-02  6.40023276e-02 -1.25724882e-01 -8.45907182e-02
  2.48465855e-02  1.34898871e-02  5.55629060e-02 -2.93893870e-02
  2.66429689e-02  5.07843420e-02 -1.18942000e-03 -1.09014109e-01
  1.42404446e-02 -6.30987734e-02  4.47497331e-02 -3.49189155e-02
  3.93742956e-02  1.80439465e-02 -4.82039526e-02  1.15270421e-01
 -1.40792519e-01 -8.29446539e-02  3.17647420e-02 -6.87950402e-02
  2.28418014e-03  1.05707645e-02 -1.77035090e-02  6.14538752e-02
  5.65050542e-02  5.83483838e-02  3.36036831e-02 -7.81908408e-02
  4.88184094e-02 -2.30752062e-02 -6.04354776e-02 -8.41501579e-02
  4.49783877e-02  5.46325967e-02 -5.16212806e-02  7.21440767e-04
  4.17707190e-02  2.89895274e-02  5.72448503e-03  3.40613797e-02
 -1.68883204e-02 -9.43698063e-02  4.41694260e-03 -5.72452322e-02
  6.95688557e-03 -4.72279228e-02  5.50267939e-03  4.24454547e-02
 -2.06548814e-02  4.80774557e-03  4.01677471e-03  1.35990873e-01
 -1.70331616e-02 -5.26663428e-03 -5.73393982e-03  9.73371640e-02
  7.63154924e-02  2.71088313e-02  1.15384959e-01 -8.60132352e-02
  6.24583149e-03 -1.76615249e-02  1.49291614e-02 -3.23114358e-02
  1.20315798e-01 -1.37789743e-02 -8.84567015e-03  2.89110038e-02
 -3.97716612e-02  3.87413166e-02 -4.51677293e-02 -1.99127011e-02
  7.55467489e-02 -1.02538811e-02  5.05392365e-02  7.40993842e-02
 -5.52552752e-02 -8.12891200e-02 -1.27333552e-01  9.85728502e-02
 -3.42262117e-03  7.89161995e-02  7.80440196e-02  1.86082441e-02
  1.12325167e-02 -1.78675633e-02  4.59727012e-02 -6.15675293e-04
  2.82679386e-02  4.67753550e-03  6.36407137e-02  5.93631752e-02
  9.21989232e-02 -1.61986835e-02 -1.96268372e-02  6.67902604e-02
 -5.10301739e-02 -4.22006585e-02 -8.15445557e-03  8.30097571e-02
 -4.52641062e-02 -2.43692249e-02 -7.55409244e-03 -4.52204682e-02
 -2.43624509e-03 -1.28824618e-02 -2.17346139e-02  4.66412685e-33
 -1.92291010e-02  3.38959768e-02  3.02156042e-02 -4.38046977e-02
  8.15876853e-03 -4.60115634e-02 -5.31844422e-03  3.41555104e-02
 -6.64872676e-02 -2.31052916e-02 -2.03916319e-02 -1.87411513e-02
 -6.49014935e-02  1.46500826e-01 -1.07832234e-02 -2.74298601e-02
 -3.59992571e-02  6.53540492e-02 -2.21362486e-02 -8.84237408e-04
  7.90866613e-02 -3.85905020e-02 -6.55978844e-02 -6.07736744e-02
  3.70099396e-03 -7.66619528e-03  9.32073966e-02 -6.14669733e-02
 -6.73083542e-03  1.01368325e-02 -1.30949453e-01 -3.43734138e-02
 -2.12249234e-02  4.29290347e-02  1.55341523e-02 -5.71325496e-02
 -2.98370738e-02 -4.55812961e-02 -6.02531573e-03 -1.21636903e-02
  4.42578178e-03  6.30749241e-02  1.62855070e-02 -2.60945447e-02
 -8.20359215e-02 -5.87974489e-02  3.03453929e-03 -2.76655760e-02
 -2.90018488e-02  8.78183730e-03  1.61610283e-02 -3.56829725e-02
 -2.37761959e-02 -5.01886085e-02  3.18017900e-02 -3.44655588e-02
  5.44532128e-02  9.16674659e-02  4.57444564e-02  3.14397924e-02
  7.44730700e-03  2.81543043e-02  1.57230198e-02  1.13029681e-01
  8.45999457e-03  6.37475401e-02 -9.57022235e-03  1.46056086e-01
  1.61170159e-02  2.47888695e-02 -8.03040108e-04  1.64081901e-02
  3.95082794e-02 -2.65461002e-02 -6.04649400e-03  1.16666509e-02
  4.71573919e-02 -1.01466395e-01  1.52230738e-02  3.13782133e-02
  1.87015627e-02  6.05213754e-02 -2.29711961e-02  2.80696284e-02
 -1.19734667e-02 -3.50229107e-02  6.42545661e-03 -8.83941054e-02
  1.88683011e-02  1.53513495e-02 -2.00652312e-02 -4.57691029e-02
 -3.07951011e-02 -2.99283792e-03 -7.25531951e-02 -2.80538247e-33
 -1.79764684e-04  6.79054558e-02 -4.98998761e-02  6.09748997e-02
 -3.32328528e-02 -1.72711425e-02 -4.13232148e-02  5.03201224e-02
 -6.12706952e-02  1.30293453e-02  8.57346412e-03 -2.62614526e-02
  6.47765845e-02 -2.39429325e-02  6.25177622e-02 -1.66654326e-02
 -2.22485978e-02  8.12773127e-03  1.64038781e-02 -1.03466976e-02
  3.06867957e-02  6.58852011e-02 -7.84937888e-02  6.46218359e-02
 -4.69757728e-02 -1.86147522e-02 -5.43937646e-02  7.00666085e-02
 -5.85723389e-03 -1.42592015e-02 -5.12145320e-03 -1.62344202e-02
 -2.01102979e-02  3.34333964e-02 -1.92594007e-02  3.36539783e-02
  6.61960766e-02 -1.80941932e-02 -1.77126769e-02  3.14083807e-02
  1.13496102e-01 -5.68226688e-02 -1.65281985e-02  2.57995017e-02
 -1.08513109e-01  1.69490129e-02 -5.26490510e-02 -4.25188169e-02
 -4.67196247e-03  1.56409405e-02 -2.20717955e-02 -7.89551623e-03
 -8.79504457e-02 -1.41689717e-03 -7.41649270e-02 -6.89151213e-02
  1.32033322e-03  7.18301814e-03  4.22402807e-02 -1.29507603e-02
 -5.94598167e-02 -2.15145312e-02  4.72502075e-02 -4.47048321e-02
  1.82686951e-02 -3.41442935e-02 -2.28103846e-02  3.97903956e-02
  2.98106577e-02 -1.37345148e-02  1.47424899e-02 -8.66568554e-03
  6.74800426e-02  9.42609981e-02 -6.54760236e-03  6.23168871e-02
  5.53143248e-02 -7.30767623e-02 -1.26313828e-02 -3.48973498e-02
  3.29142367e-03  1.93192922e-02 -5.01433993e-03  4.39123958e-02
  6.30708411e-02  7.76557699e-02  1.89034715e-02  6.32455796e-02
 -2.61266027e-02  4.19539623e-02 -6.05344698e-02  5.59334494e-02
 -4.70450446e-02  1.07230552e-01 -4.37843017e-02 -4.39244481e-08
 -4.54437286e-02  5.85300922e-02  3.94487828e-02  3.78576294e-02
  1.00979740e-02 -7.13360757e-02 -2.50397399e-02  9.42402035e-02
 -4.34394786e-03  4.68532480e-02  9.48943794e-02 -1.58426519e-02
 -5.22853658e-02 -6.73166960e-02  2.37378310e-02  6.27046078e-02
  2.45904513e-02  1.48554286e-02 -7.40340119e-03 -5.34285381e-02
  6.22455850e-02  4.06174846e-02  7.12229013e-02 -7.54844472e-02
  5.87192662e-02 -7.54837692e-02 -2.89884228e-02  8.29493254e-02
 -1.11948736e-02 -3.05847302e-02  6.50366477e-04  2.79335175e-02
 -2.71966998e-02 -9.44097117e-02  2.87865531e-02  1.28622890e-01
 -2.21352205e-02 -2.42820680e-02  5.68572134e-02 -2.05307454e-03
  2.89760586e-02  1.02703713e-01 -2.61279680e-02  1.42625729e-02
 -1.83888283e-02 -6.10382203e-03 -5.25136627e-02 -4.78076264e-02
  5.91903441e-02 -3.91052254e-02 -7.87660852e-03  5.99236740e-03
  1.03718638e-02  9.41201523e-02  2.99628731e-02  5.04732020e-02
 -3.61867733e-02 -1.43786683e-03  3.96512300e-02  5.43300994e-02
  6.14603162e-02 -7.25980178e-02 -4.58648242e-02  2.27013156e-02]
ChromaDB papildinata



Token count: 1502






VICTORY

[OUTPUT]
Q: Can you give me a technical report on DeepSeekv3?
A: Here is the technical report on DeepSeek-V3:

**DeepSeek-V3 Technical Report**

**Abstract**
We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance.

**Introduction**
DeepSeek-V3 is designed to achieve high-performance language processing capabilities while being cost-effective and efficient. It builds upon the previous work of DeepSeek-V2 and introduces new architectures and strategies for improved performance.

**Architecture**

### Basic Architecture
The basic architecture of DeepSeek-V3 consists of a combination of Multi-head Latent Attention (MLA) and DeepSeekMoE models, which were previously validated in DeepSeek-V2.

### Multi-Head Latent Attention (MLA)
MLA is a novel attention mechanism that allows for efficient inference and training. It uses multiple heads to attend to different token representations, leading to improved performance.

### DeepSeekMoE with Auxiliary-Loss-Free Load Balancing
DeepSeekMoE is an architecture that utilizes MoEs to process input tokens. The auxiliary-loss-free load balancing strategy helps to distribute the computation load evenly across the model, improving training efficiency and stability.

**Multi-Token Prediction**
The multi-token prediction objective involves predicting multiple tokens simultaneously, which allows for stronger performance and improved language understanding.

**Infrastructures**

### Compute Clusters
DeepSeek-V3 was trained on a cluster of compute nodes using a distributed computing framework. This allowed for efficient scaling and parallelization of the training process.

### Training Framework
The training framework used in DeepSeek-V3 includes DualPipe, which enables computation-communication overlap, and an efficient implementation of cross-node all-to-all communication. Additionally, the framework uses memory-saving techniques with minimal overhead to reduce computational requirements.

### FP8 Training
DeepSeek-V3 was trained using the FP8 format, which allows for mixed precision training and reduced memory usage. This enabled the model to train on large datasets while minimizing computational costs.

**Inference and Deployment**

### Prefilling
The prefiling technique involves pre-computing and storing common token combinations, allowing for faster inference times and improved performance.

### Decoding
The decoding process in DeepSeek-V3 uses a combination of attention mechanisms and MoEs to generate output tokens.

**Suggestions on Hardware Design**
To achieve optimal performance with DeepSeek-V3, we recommend designing hardware that prioritizes high-bandwidth communication and low-latency computation. This can be achieved through the use of specialized chips or accelerators designed specifically for language processing tasks.

**Pre-Training**

### Data Construction
The pre-training data used in DeepSeek-V3 consists of a large corpus of text data, which was constructed using a combination of web scraping, book downloads, and other sources.

### Hyper-Parameters
The hyperparameters used in the training process included batch sizes, learning rates, and number of epochs. The optimal values for these parameters were determined through experimentation and validation.

### Long Context Extension
DeepSeek-V3 includes a long context extension module that allows it to process longer input sequences and capture longer-range dependencies.

**Evaluations**
The performance of DeepSeek-V3 was evaluated on several benchmarks, including language translation tasks, question answering, and text generation. The results show significant improvements over previous models, demonstrating the effectiveness of the proposed architecture and training strategies.