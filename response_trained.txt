**DeepSeek-V3 Technical Report**

**Abstract**
We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters and 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.

**Architecture**

### Basic Architecture

DeepSeek-V3 is based on a Mixture-of-Experts (MoE) architecture, which consists of:

* **Multi-Head Latent Attention (MLA)**: A novel attention mechanism that enables parallelized computation and efficient inference.
* **DeepSeekMoE with Auxiliary-Loss-Free Load Balancing**: A modified MoE architecture that balances the load between experts without requiring an auxiliary loss function.

### Multi-Token Prediction

DeepSeek-V3 uses a multi-token prediction objective, which allows for stronger performance by predicting multiple tokens simultaneously.

**Infrastructures**

### Compute Clusters

DeepSeek-V3 was trained on a compute cluster consisting of:

* DualPipe: A novel framework that enables computation-communication overlap and reduces training time.
* Extremely Memory Saving with Minimal Overhead: An optimized memory management strategy that minimizes the overhead of storing model weights.

### Training Framework

The training framework consists of:

* **DualPipe**: Enables computation-communication overlap and reduces training time.
* **Efficient Implementation of Cross-Node All-to-All Communication**: Optimizes communication between nodes in the cluster.
* **Mixed Precision Framework**: Allows for efficient training with mixed precision arithmetic.

### FP8 Training

DeepSeek-V3 uses a Mixed Precision Framework that enables training with reduced precision, which reduces memory usage and improves training speed.

### Inference and Deployment

Inference is performed using:

* **Prefilling**: Prefills the cache to reduce the number of memory accesses.
* **Decoding**: Decodes the output of the model.

**Suggestions on Hardware Design**

To improve performance, consider:

* **Communication Hardware**: Optimizing communication hardware to reduce latency and increase throughput.
* **Compute Hardware**: Optimizing compute hardware to increase processing power and reduce energy consumption.

**Pre-Training**

### Data Construction

The pre-training data consists of 14.8 trillion diverse and high-quality tokens.

### Hyper-Parameters

The hyper-parameters used for pre-training include:

* Learning rate: 0.0001
* Batch size: 128
* Number of epochs: 10

### Long Context Extension

DeepSeek-V3 uses a long context extension to enable the model to predict longer sequences.

### Evaluations

The evaluations show that DeepSeek-V3 outperforms previous models on various benchmarks, achieving state-of-the-art results.